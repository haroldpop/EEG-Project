{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da53a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f05cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d13371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful functions\n",
    "\n",
    "def import_matfiles(path):\n",
    "    list_mat_ex = os.listdir(path2)\n",
    "    #list_mat_file contains all interessant files for our research\n",
    "    list_mat_ex.remove('readme.txt')\n",
    "    #list_mat_file contains all interessant files for our research\n",
    "    label = list_mat_ex[-1]\n",
    "    print(label)\n",
    "    list_mat_ex.pop(-1)\n",
    "    return list_mat_ex, label\n",
    "    \n",
    "def find_max_shape(datas, freq = None):\n",
    "    #shape of data in datas (freq, nb channel, activity)\n",
    "    if freq == None:\n",
    "        max_ = datas[0].shape[2]\n",
    "        for data in datas[1:]:\n",
    "            act_shape = data.shape[2]\n",
    "            if act_shape > max_ :\n",
    "                max_ = act_shape\n",
    "    else :\n",
    "        #hape of data in datas (nb channel, activity)\n",
    "        max_ = datas[0].shape[1]\n",
    "        for data in datas[1:]:\n",
    "            act_shape = data.shape[1]\n",
    "            if act_shape > max_ :\n",
    "                max_ = act_shape\n",
    "    return max_  \n",
    "\n",
    "def x_shaper(x):\n",
    "    \"\"\"\n",
    "    Arg: x as when it outcome from the dataset\n",
    "    return: x as we wants tensor with (freq, nb_channel, activity)\n",
    "    \"\"\"\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    return torch.from_numpy(x).type(torch.float32)\n",
    "\n",
    "def datas_shaper(datas):\n",
    "    res = []\n",
    "    for data in datas:\n",
    "        print(data.shape)\n",
    "        res.append(x_shaper(data))\n",
    "    return res \n",
    "\n",
    "def padd(datas, freq = None):\n",
    "    #shape of data in datas (freq, nb channel activity)\n",
    "    max_shape = find_max_shape(datas, freq =freq)\n",
    "    res = []\n",
    "    if freq == None :\n",
    "        for data in datas:\n",
    "            aux = torch.zeros((data.shape[0], data.shape[1], max_shape))\n",
    "            if data.shape[2] < max_shape:\n",
    "                add_shape = max_shape - data.shape[2]\n",
    "                for freq in range(data.shape[0]):\n",
    "                    pad_tensor = torch.zeros((data.shape[1], add_shape))\n",
    "                    aux[freq, :, :] = torch.cat((data[freq, :, :], pad_tensor), dim=1)\n",
    "                    aux = aux.type(torch.float32)\n",
    "                res.append(aux)\n",
    "            else :\n",
    "                res.append(data.type(torch.float32))\n",
    "    else :\n",
    "        for data in datas:\n",
    "            aux = torch.zeros((data.shape[0], max_shape))\n",
    "            if data.shape[1] < max_shape:\n",
    "                add_shape = max_shape - data.shape[1]\n",
    "                pad_tensor = torch.zeros((data.shape[0], add_shape))\n",
    "                aux = torch.cat((data, pad_tensor), dim=1)\n",
    "                aux = aux.type(torch.float32)\n",
    "                res.append(aux)\n",
    "            else :\n",
    "                res.append(data.type(torch.float32))\n",
    "    return res\n",
    "\n",
    "def normalize(x, freq = None):\n",
    "    if freq == None:\n",
    "        for frequencies in range(x.shape[0]):\n",
    "            mean_ = torch.mean(x[frequencies, :, :], 1).reshape(-1, 1)\n",
    "            std_ = torch.std(x[frequencies, : ,: ], 1).reshape(-1, 1)\n",
    "            x[frequencies, :, :] = (x[frequencies, :, :] - mean_) / std_\n",
    "            return x\n",
    "    else :\n",
    "        mean_ = torch.mean(x[freq, :, :], 1).reshape(-1, 1)\n",
    "        std_ = torch.std(x[freq, : ,: ], 1).reshape(-1, 1)\n",
    "        x[freq, :, :] = (x[freq, :, :] - mean_) / std_\n",
    "        return x[freq, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ccefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version to improve in a near future..\n",
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, path, signals, subject_number, experiment_number, labels, smoothing_method, freq = None):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            path: path to folder with all the .mat files of the dataset\n",
    "            signals: feature that we want to extract\n",
    "            subject_number: subject of the experimenr\n",
    "            epxeriment_number: number of the experiment, like 0 week 1 week or 2 week \n",
    "            (MAYBE I MISUNDERSTOOD THIS PART READ AGAIN DATASET DETAILS)\n",
    "            labels: labels list\n",
    "            smoothing method: movingAve or LDS\n",
    "            freq: 0, 1, 2, 3 or 4 for just specific one and None if you want them all\n",
    "        \"\"\"\n",
    "        list_file, _ = import_matfiles(path)\n",
    "        #selection of interesting files that means the one related to the subject\n",
    "        list_sub_file = []\n",
    "        for file in list_file:\n",
    "            num = file.split('_')[0]\n",
    "            if int(num) == subject_number: \n",
    "                list_sub_file.append(file)\n",
    "             \n",
    "        working_file = list_sub_file[experiment_number]\n",
    "        \n",
    "        dic = loadmat(path + '\\\\' + working_file) \n",
    "        if smoothing_method == 'movingAve':\n",
    "            smooth = signals.lower() + '_movingAve'\n",
    "        elif smooting_method == 'LDS':\n",
    "            smooth = signals.lower() + '_LDS'\n",
    "        else :\n",
    "            raise ValueError('Please select a good smoothing method: movingAve or LDS')\n",
    "        \n",
    "        #maybe a list is not the best tool to store it\n",
    "        datas = []\n",
    "        for k in range(15):\n",
    "            sig = dic[smooth + str(k+1)]\n",
    "            sig = x_shaper(sig)\n",
    "            sig = normalize(sig, freq = freq) #change the freq if needed!!!!!!!!!!!!\n",
    "            datas.append(sig)\n",
    "            \n",
    "        \n",
    "        datas = padd(datas, freq = freq) #everything is happening in this function, the changement from Long to Float\n",
    "                                    #and the slicing for the frequency we want\n",
    "\n",
    "        labels = labels.reshape(-1, )\n",
    "        labels = labels + 1 #sliding to use CrossEntropy Pytorch function\n",
    "        labels = torch.from_numpy(labels)\n",
    "                                  \n",
    "                                  \n",
    "        self.datas = datas\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.datas[idx], self.labels[idx].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b9ffb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label.mat\n"
     ]
    }
   ],
   "source": [
    "#get labels \n",
    "path2 = r'C:\\Users\\harol\\UsefulCode\\CP_DSAI\\Project\\Dataset\\SEED\\ExtractedFeatures'\n",
    "l, label_ = import_matfiles(path2)\n",
    "labels = loadmat(path2 + '\\\\' + label_)['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d5ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label.mat\n",
      "15\n",
      "9 6\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\harol\\UsefulCode\\CP_DSAI\\Project\\Dataset\\SEED\\ExtractedFeatures'\n",
    "signals = 'de'\n",
    "subject_number = 4\n",
    "experiment_number = 0\n",
    "labels = labels\n",
    "smoothing_method = 'movingAve'\n",
    "dataset = SignalDataset(path, signals, subject_number, experiment_number, labels, smoothing_method, freq = 0)\n",
    "train_dataset, test_dataset = random_split(dataset, [9, 6])\n",
    "print(len(dataset))\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a55f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2232f2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([1, 62, 265])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "samp = next(iter(train_loader))\n",
    "data, lab = samp\n",
    "print(data.dtype)\n",
    "print(data.shape)\n",
    "print(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b2739e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 62, 62])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#creation of the adjency matrix\n",
    "\n",
    "channel = ['FP1', 'FPZ', 'FP2', 'AF3', 'AF4', 'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8', \n",
    "                'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6','FT8','T7','C5','C3','C1',\n",
    "                'CZ','C2','C4','C6', 'T8','TP7','CP5','CP3','CP1','CPZ','CP2','CP4','CP6','TP8','P7',\n",
    "                'P5','P3','P1','PZ','P2','P4','P6','P8','PO7','PO5','PO3','POZ','PO4','PO6','PO8','CB1',\n",
    "                'O1','OZ','O2','CB2']\n",
    "idx = np.arange(62)\n",
    "df_channel = pd.DataFrame({'idx': idx, 'channel': channel })\n",
    "\n",
    "#initialization of the adjency matrix\n",
    "feature_number = 62\n",
    "W = np.zeros((5, feature_number, feature_number))\n",
    "for freq in range(5):\n",
    "    #I don't see other way than initialiazing W manually\n",
    "    #was done accodingly to the EEG channel map and order, hope to don't have make mistake\n",
    "    #FP1\n",
    "    W[freq, 0, 1] = 1\n",
    "    W[freq, 0, 3] = 1\n",
    "    #FPZ\n",
    "    W[freq, 1, 0] = 1\n",
    "    W[freq, 1, 2] = 1\n",
    "    #FP2\n",
    "    W[freq, 2, 1] = 1\n",
    "    W[freq, 2, 4] = 1\n",
    "    #AF3\n",
    "    W[freq, 3, 8] = 1\n",
    "    W[freq, 3, 7] = 1\n",
    "    W[freq, 3, 6] = 1\n",
    "    W[freq, 3, 0] = 1\n",
    "    #AF4\n",
    "    W[freq, 4, 2] = 1\n",
    "    W[freq, 4, 10] = 1\n",
    "    W[freq, 4, 11] = 1\n",
    "    W[freq, 4, 12] = 1\n",
    "    #F7\n",
    "    W[freq, 5, 6] = 1\n",
    "    W[freq, 5, 14] = 1\n",
    "    #F5\n",
    "    W[freq, 6, 3] = 1\n",
    "    W[freq, 6, 5] = 1\n",
    "    W[freq, 6, 15] = 1\n",
    "    W[freq, 6, 7] = 1\n",
    "    #F3\n",
    "    W[freq, 7, 3] = 1\n",
    "    W[freq, 7, 8] = 1\n",
    "    W[freq, 7, 16] = 1\n",
    "    #F1\n",
    "    W[freq, 8, 9] = 1\n",
    "    W[freq, 8, 17] = 1\n",
    "    #FZ\n",
    "    W[freq, 9, 10] = 1\n",
    "    W[freq, 9, 18] = 1\n",
    "    #F2\n",
    "    W[freq, 10, 11] = 1\n",
    "    W[freq, 10, 19] = 1\n",
    "    #F4\n",
    "    W[freq, 11, 12] = 1\n",
    "    W[freq, 11, 20] = 1\n",
    "    #F6 \n",
    "    W[freq, 12, 13] = 1\n",
    "    W[freq, 12, 21] = 1\n",
    "    #F8\n",
    "    W[freq, 13, 22] = 1\n",
    "    #FT7\n",
    "    W[freq, 14, 15] = 1\n",
    "    W[freq, 14, 23] = 1\n",
    "    #FC5 to TP8\n",
    "    for k in range(15, 41): #till TP8 (number 40) we have two new neigbors: on the right and under +1 and +9 (nine electrodes per line)\n",
    "        if k == 22 or k == 31 or k == 40 :\n",
    "            W[freq, k, k+9] = 1\n",
    "        else:\n",
    "            W[freq, k, k+1] = 1\n",
    "            W[freq, k, k+9] = 1\n",
    "    #P7\n",
    "    W[freq, 41, 42] = 1\n",
    "    W[freq, 41, 50] = 1\n",
    "    #P5\n",
    "    W[freq, 42, 43] = 1\n",
    "    W[freq, 42, 51] = 1\n",
    "    #P3\n",
    "    W[freq, 43, 44] = 1\n",
    "    #P1\n",
    "    W[freq, 44, 45] = 1\n",
    "    W[freq, 44, 52] = 1\n",
    "    #PZ\n",
    "    W[freq, 45, 46] = 1\n",
    "    W[freq, 45, 53] = 1\n",
    "    #P2\n",
    "    W[freq, 46, 47] = 1\n",
    "    W[freq, 46, 54] = 1\n",
    "    #P4\n",
    "    W[freq, 47, 48] = 1\n",
    "    #P6\n",
    "    W[freq, 48, 49] = 1\n",
    "    W[freq, 48, 55] = 1\n",
    "    #P8\n",
    "    W[freq, 49, 56] = 1\n",
    "    #PO7\n",
    "    W[freq, 50, 51] = 1\n",
    "    W[freq, 50, 57] = 1\n",
    "    #PO5\n",
    "    W[freq, 51, 52] = 1\n",
    "    W[freq, 50, 57] = 1\n",
    "    #PO3\n",
    "    W[freq, 52, 53] = 1\n",
    "    W[freq, 52, 58] = 1\n",
    "    #POZ\n",
    "    W[freq, 53, 54] = 1\n",
    "    W[freq, 53, 59] = 1\n",
    "    #PO4\n",
    "    W[freq, 54, 55] = 1\n",
    "    W[freq, 54, 60] = 1\n",
    "    #PO6\n",
    "    W[freq, 55, 56] = 1\n",
    "    W[freq, 55, 61] = 1\n",
    "    #PO8\n",
    "    W[freq, 56, 61] = 1\n",
    "    #CB1\n",
    "    W[freq, 57, 58] = 1\n",
    "    #O1\n",
    "    W[freq, 58, 59] = 1\n",
    "    #OZ\n",
    "    W[freq, 59, 60] = 1\n",
    "    #O2\n",
    "    W[freq, 60, 61] = 1\n",
    "\n",
    "for freq in range(5):\n",
    "    for i in range(feature_number):\n",
    "        for j in range(i, feature_number):\n",
    "            if W[freq, j, i] == 0:\n",
    "                W[freq, j, i] = W[freq, i, j]\n",
    "    W[freq, :, :] = W[freq, :, :] - np.eye(feature_number)\n",
    "\n",
    "#for some training stability reasons we initialized the diagonal of W with 1\n",
    "for freq in range(5):\n",
    "    W[freq, :, :] = W[freq, :, :] + np.eye(W.shape[1], dtype=int)\n",
    "W = torch.from_numpy(W).type(torch.float32)\n",
    "print(W.shape)\n",
    "print(W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17cc9b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions use inside the model\n",
    "\n",
    "##### CREATE A CLASS IF IMPLEMENTATION FINISHED ####\n",
    "#more elegant\n",
    "\n",
    "\n",
    "def D_matrix(W, multifreq = True):\n",
    "    if multifreq:\n",
    "        for freq in range(W.shape[0]):\n",
    "            D[freq, :, :] = torch.diag(W[freq].sum(dim=-1))\n",
    "    else :\n",
    "        D = torch.diag(W.sum(dim=-1))\n",
    "    return D\n",
    "\n",
    "def laplacian(W, multifreq = True): #here we calculate the non noramlized Laplacian matrix of the graph\n",
    "    #multifreq means if we get the five frequencies in one or not\n",
    "    L = D_matrix(W, multifreq = multifreq) - W\n",
    "    L = L.type(torch.float32)\n",
    "    return L\n",
    "\n",
    "\n",
    "def x_shaper(x):\n",
    "    \"\"\"\n",
    "    Arg: x as when it outcome from the dataset\n",
    "    return: x as we wants tensor with (freq, nb_channel, activity)\n",
    "    \"\"\"\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    return torch.from_numpy(x)\n",
    "\n",
    "\n",
    "def polynomial_approximation(L, x, theta, multifreq = True):\n",
    "    \"\"\"\n",
    "    Arg: Laplacian matrix, x the extracted signal, theta chebyshev coefficients, K the order of the polynom\n",
    "    \n",
    "    we suppose that x was transform to a tensor and that its shape was made as (freq, nb channel, activity)\n",
    "    \n",
    "    Return: the U*sum theta[k]*lambda**k*U^{T}\n",
    "    \"\"\"\n",
    " \n",
    "    y = torch.zeros_like(x) \n",
    "    K = theta.shape[0]\n",
    "    \n",
    "    if multifreq :\n",
    "        L_tilde = torch.zeros_like(L) \n",
    "        for freq in range(L.shape[0]):\n",
    "            with torch.no_grad():\n",
    "                eigenvalues, eigenvectors = torch.linalg.eig(L[freq, :, :])\n",
    "                eigenvalues = eigenvalues.real #since L is symmetric real L is diagonalizable in the real space\n",
    "                U = eigenvectors.real\n",
    "        \n",
    "            eigen_values_diag = torch.diag(eigenvalues)\n",
    "            aux = torch.zeros_like(eigen_values_diag)\n",
    "            for k in range(K):\n",
    "                aux += torch.linalg.matrix_power(eigen_values_diag, k)*theta[k]\n",
    "            y[freq, :, :] = torch.matmul(torch.matmul(U, torch.matmul(aux, U.transpose(1, 0))), x)\n",
    "            \n",
    "    else :  \n",
    "        #since we don't work with chebyshev approximation there is no need to rescale the Laplacian\n",
    "        with torch.no_grad():\n",
    "                eigenvalues, eigenvectors = torch.linalg.eig(L)\n",
    "                eigenvalues = eigenvalues.real #since L is symmetric real L is diagonalizable in the real space\n",
    "                U = eigenvectors.real\n",
    "        \n",
    "        eigen_values_diag = torch.diag(eigenvalues)\n",
    "        aux = torch.zeros_like(eigen_values_diag)\n",
    "        for k in range(K):   \n",
    "            aux += torch.linalg.matrix_power(eigen_values_diag, k)*theta[k]\n",
    "        y = torch.matmul(torch.matmul(U, torch.matmul(aux, U.transpose(1, 0))), x) \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45f4ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "661fd418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNLayerPoly(nn.Module):\n",
    "    def __init__(self, W, K):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(W)\n",
    "        self.theta = nn.Parameter(torch.FloatTensor(K, 1))\n",
    "        self.theta = nn.init.uniform_(self.theta, 0, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        L = laplacian(self.W, multifreq = False)\n",
    "        X = polynomial_approximation(L, x, self.theta, multifreq = False)\n",
    "        return X\n",
    "    \n",
    "class DGCN_Poly(nn.Module):\n",
    "    def __init__(self, W, K, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.GCNLayer1 = DGCNLayerPoly(W, K)\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.GCNLayer1(x) \n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.l1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l2(x)\n",
    "        return self.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6748f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Model architecture ---------------\n",
      "<bound method Module.parameters of DGCN_Poly(\n",
      "  (GCNLayer1): DGCNLayerPoly()\n",
      "  (l1): Linear(in_features=16430, out_features=50, bias=True)\n",
      "  (l2): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (tanh): Tanh()\n",
      ")>\n",
      " \n",
      "------------- Model parameters ---------------\n",
      "<class 'torch.Tensor'> torch.Size([62, 62])\n",
      "<class 'torch.Tensor'> torch.Size([10, 1])\n",
      "<class 'torch.Tensor'> torch.Size([50, 16430])\n",
      "<class 'torch.Tensor'> torch.Size([50])\n",
      "<class 'torch.Tensor'> torch.Size([3, 50])\n",
      "<class 'torch.Tensor'> torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "#matrix we will work with\n",
    "W_ = W[0].type(torch.float32).clone().detach().requires_grad_(True) #work with the first brain frequency\n",
    "\n",
    "K = 10\n",
    "#model\n",
    "model = DGCN_Poly(W_, K, 62*265, 50, 3).to(device)\n",
    "print('------------- Model architecture ---------------')\n",
    "print(model.parameters)\n",
    "print(' ')\n",
    "print('------------- Model parameters ---------------')\n",
    "for param in model.parameters():\n",
    "    print(type(param.data), param.size())\n",
    "    \n",
    "#hyperparameters, loss functions and optimizer\n",
    "\n",
    "learning_rate = 0.0001\n",
    "alpha = 1e-7 #regularization parameter\n",
    "num_epochs = 1000\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4d65e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000, Tr Loss: 0.6976, Tr Acc: 88.8889, Val Loss: 1.0575, Val Acc: 66.6667\n",
      "Epoch 100/1000, Tr Loss: 0.5446, Tr Acc: 100.0000, Val Loss: 1.0623, Val Acc: 66.6667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-239c501738c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m)\u001b[0m      \u001b[1;31m# Do the forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Calculate the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-db148eea5e79>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGCNLayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-db148eea5e79>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlaplacian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultifreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolynomial_approximation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultifreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-73d4c39df15d>\u001b[0m in \u001b[0;36mpolynomial_approximation\u001b[1;34m(L, x, theta, multifreq)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m#since we don't work with chebyshev approximation there is no need to rescale the Laplacian\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                 \u001b[0meigenvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meigenvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m                 \u001b[0meigenvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meigenvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m \u001b[1;31m#since L is symmetric real L is diagonalizable in the real space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meigenvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training of the model\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t1 = time.time()\n",
    "    \n",
    "    ############################\n",
    "    # Train\n",
    "    ############################\n",
    "    \n",
    "    iter_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    model.train()        # Put the network into training mode\n",
    "    \n",
    "    for i, (signal, label_) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "        outputs = model(signal)      # Do the forward pass\n",
    "        loss = criterion(outputs, label_) # Calculate the loss\n",
    "        iter_loss += loss.item() # Accumulate the loss\n",
    "\n",
    "        #L1-Regularization\n",
    "        l1_norm = sum(abs(p).sum() for p in model.parameters())\n",
    "        loss = loss + alpha*l1_norm\n",
    "        loss.backward()           # Calculate the gradients with help of back propagation\n",
    "       #print('W')\n",
    "       #print(model.GCNLayer1.W)\n",
    "       #print(model.GCNLayer1.W.grad)\n",
    "       #print('theta')\n",
    "       #print(model.GCNLayer1.theta)\n",
    "       #print(model.GCNLayer1.theta.grad)\n",
    "        optimizer.step()          # Adjust the parameters based on the gradients\n",
    "        \n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        correct += (predicted == label_).sum()\n",
    "        iterations += 1\n",
    "    \n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss / iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append((100 * correct / float(len(train_dataset))))\n",
    "   \n",
    "\n",
    "    ############################\n",
    "    # Test\n",
    "    ############################\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()                    # Put the network into evaluate mode\n",
    "    \n",
    "    for i, (signal, label_) in enumerate(test_loader):\n",
    "        \n",
    "        outputs = model(signal)      # Do the forward pass\n",
    "        loss += criterion(outputs, label_).item() # Calculate the loss\n",
    "        \n",
    "        # Record the correct predictions for training data\n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        correct += (predicted == label_).sum()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the validation loss\n",
    "    valid_loss.append(loss / iterations)\n",
    "    # Record the validation accuracy\n",
    "    correct_scalar = np.array([correct.clone()])[0]\n",
    "    valid_accuracy.append(correct_scalar.item() / len(test_dataset) * 100.0)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f'\n",
    "               %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "                 valid_loss[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ee126",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting of the loss\n",
    "x_plot = np.arange(num_epochs)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "ax[0].plot(x_plot, train_loss, label = 'train loss')\n",
    "ax[0].plot(x_plot, valid_loss, label = 'test loss')\n",
    "ax[0].set_title('Loss with epochs')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(x_plot, train_accuracy, label = 'train accuracy')\n",
    "ax[1].plot(x_plot, valid_accuracy, label = 'test accuracy')\n",
    "ax[1].set_title('Accuracy with epochs')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d22ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, train_loader, test_loader, optimizer, loss_function, alpha):\n",
    "\n",
    "    #training of the model\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        ############################\n",
    "        # Train\n",
    "        ############################\n",
    "\n",
    "        iter_loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.train()        # Put the network into training mode\n",
    "\n",
    "        for i, (signal, label_) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "            outputs = model(signal)      # Do the forward pass\n",
    "            loss = criterion(outputs, label_) # Calculate the loss\n",
    "            iter_loss += loss.item() # Accumulate the loss\n",
    "\n",
    "            #L1-Regularization\n",
    "            l1_norm = sum(abs(p).sum() for p in model.parameters())\n",
    "            loss = loss + alpha*l1_norm\n",
    "            loss.backward()           # Calculate the gradients with help of back propagation\n",
    "           #print('W')\n",
    "           #print(model.GCNLayer1.W)\n",
    "           #print(model.GCNLayer1.W.grad)\n",
    "           #print('theta')\n",
    "           #print(model.GCNLayer1.theta)\n",
    "           #print(model.GCNLayer1.theta.grad)\n",
    "            optimizer.step()          # Adjust the parameters based on the gradients\n",
    "\n",
    "\n",
    "            # Record the correct predictions for training data \n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            correct += (predicted == label_).sum()\n",
    "            iterations += 1\n",
    "\n",
    "        # Record the training loss\n",
    "        train_loss.append(iter_loss / iterations)\n",
    "        # Record the training accuracy\n",
    "        train_accuracy.append((100 * correct / float(len(train_dataset))))\n",
    "\n",
    "\n",
    "        ############################\n",
    "        # Test\n",
    "        ############################\n",
    "\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.eval()                    # Put the network into evaluate mode\n",
    "\n",
    "        for i, (signal, label_) in enumerate(test_loader):\n",
    "\n",
    "            outputs = model(signal)      # Do the forward pass\n",
    "            loss += criterion(outputs, label_).item() # Calculate the loss\n",
    "\n",
    "            # Record the correct predictions for training data\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            correct += (predicted == label_).sum()\n",
    "\n",
    "            iterations += 1\n",
    "\n",
    "        # Record the validation loss\n",
    "        valid_loss.append(loss / iterations)\n",
    "        # Record the validation accuracy\n",
    "        correct_scalar = np.array([correct.clone()])[0]\n",
    "        valid_accuracy.append(correct_scalar.item() / len(test_dataset) * 100.0)\n",
    "        \n",
    "    return train_loss, valid_loss, train_accuracy, valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f073948",
   "metadata": {},
   "source": [
    "## Time of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bfc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_exec = np.zeros(10)\n",
    "\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "alpha = 1e-7\n",
    "\n",
    "exp_time_train_loss = []\n",
    "exp_time_test_loss = []\n",
    "exp_time_train_acc = []\n",
    "exp_time_test_acc = []\n",
    "\n",
    "K = 12\n",
    "\n",
    "for k in range(10):\n",
    "    \n",
    "    W_ = W[0].type(torch.float32).clone().detach().requires_grad_(True) #work with the first brain frequency\n",
    "\n",
    "    model = DGCN_Poly(W_, K, 62*265, 50, 3).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9) \n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    train_loss, valid_loss, train_accuracy, valid_accuracy = train_model(model, num_epochs, \n",
    "                                                                train_loader, test_loader, optimizer, \n",
    "                                                                         criterion, alpha)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    time_exec[k] = t2 - t1\n",
    "    \n",
    "    exp_time_train_loss.append(train_loss)\n",
    "    exp_time_test_loss.append(valid_loss)\n",
    "    exp_time_train_acc.append(train_accuracy)\n",
    "    exp_time_test_acc.append(valid_accuracy)\n",
    "    \n",
    "print(np.mean(time_exec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a235569",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_time_train_loss = np.array(exp_time_train_loss)\n",
    "exp_time_test_loss = np.array(exp_time_test_loss)\n",
    "exp_time_train_acc = np.array(exp_time_train_acc)\n",
    "exp_time_test_acc = np.array(exp_time_test_acc)\n",
    "\n",
    "mean_exp_time_train_loss = np.mean(exp_time_train_loss, axis = 0)\n",
    "mean_exp_time_test_loss = np.mean(exp_time_test_loss, axis = 0)\n",
    "mean_exp_time_train_acc = np.mean(exp_time_train_acc, axis = 0)\n",
    "mean_exp_time_test_acc = np.mean(exp_time_test_acc, axis = 0)\n",
    "\n",
    "x_plot = np.arange(num_epochs)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "ax[0].plot(x_plot, mean_exp_time_train_loss, label = 'train loss')\n",
    "ax[0].plot(x_plot, mean_exp_time_test_loss, label = 'test loss')\n",
    "ax[1].plot(x_plot, mean_exp_time_train_acc, label = 'train accuracy')\n",
    "ax[1].plot(x_plot, mean_exp_time_test_acc, label = 'test accuracy')\n",
    "\n",
    "ax[0].set_title('Loss with epochs')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "ax[1].set_title('Accuracy with epochs')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc19af6",
   "metadata": {},
   "source": [
    "## Final Tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e2ed488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix we will work with\n",
    "W_ = W[0].type(torch.float32).clone().detach().requires_grad_(True) #work with the first brain frequency\n",
    "\n",
    "K = 10\n",
    "    \n",
    "#hyperparameters, loss functions and optimizer\n",
    "\n",
    "learning_rate = 0.0001\n",
    "alpha = 1e-7 #regularization parameter\n",
    "num_epochs = 1000\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f830b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 62, 62])\n",
      "torch.Size([5, 27, 27])\n",
      "torch.Size([5, 23, 23])\n",
      "torch.Size([5, 54, 54])\n"
     ]
    }
   ],
   "source": [
    "W_de = W\n",
    "print(W_de.shape)\n",
    "\n",
    "W_dasm = torch.zeros(5, 27, 27)\n",
    "for freq in range(5):\n",
    "    W_dasm[freq, :, :] = W_de[0, :27, :27] #we take 0 because it's all the same for every frequencies\n",
    "print(W_dasm.shape)\n",
    "\n",
    "#initialization for DCAU\n",
    "W_dcau = torch.zeros(5, 23, 23)\n",
    "for freq in range(5):\n",
    "    W_dcau[freq, :, :] = W_de[0, :23, :23] #we take 0 because it's all the same for every frequencies\n",
    "print(W_dcau.shape)\n",
    "\n",
    "#intialization for ASM \n",
    "W_asm = torch.zeros(5, 54, 54)\n",
    "for freq in range(5):\n",
    "    W_asm[freq, :, :] = W_de[0, :54, :54] #we take 0 because it's all the same for every frequencies\n",
    "print(W_asm.shape)\n",
    "\n",
    "def choose_W(feature, W_de, W_dasm, W_dcau, W_asm):\n",
    "    if feature == 'de' or feature == 'psd':\n",
    "        return W_de\n",
    "    if feature == 'dasm' or feature == 'rasm':\n",
    "        return W_dcam\n",
    "    if feature == 'dcau':\n",
    "        return W_dcau\n",
    "    if feature == 'asm':\n",
    "        return W_asm\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f14b85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label.mat\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\harol\\UsefulCode\\CP_DSAI\\Project\\Dataset\\SEED\\ExtractedFeatures'\n",
    "l, label_ = import_matfiles(path)\n",
    "labels = loadmat(path + '\\\\' + label_)['label']\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "smoothing_method = 'movingAve'\n",
    "batch_size = 1\n",
    "learning_rate = 0.0001\n",
    "alpha = 1e-7\n",
    "K = 9\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "639222dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_model_adaptator(feature, device):\n",
    "    if feature == 'de' or feature == 'psd':\n",
    "        model = DGCN_Poly(W_, K, 62*265, 50, 3).to(device)\n",
    "    elif feature == 'dasm' or feature == 'rasm':\n",
    "        model = DGCN_Poly(W_, K, 27*265, 50, 3).to(device)\n",
    "    elif feature == 'dcau':\n",
    "        model = DGCN_Poly(W_, K, 23*265, 50, 3).to(device)\n",
    "    elif feature == 'asm':\n",
    "        model = DGCN_Poly(W_, K, 54*265, 50, 3).to(device)\n",
    "    else :\n",
    "        raise TypeError(\"Select a good feature: de, psd, dasm, rasm, dcau, asm\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58b4c2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n",
      "label.mat\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0b268e1646f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                 train_loss, valid_loss, train_accuracy, valid_accuracy = train_model(model, num_epochs, \n\u001b[0m\u001b[0;32m     42\u001b[0m                                                                         \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                                                                                  criterion, alpha)\n",
      "\u001b[1;32m<ipython-input-15-3e2905ad27a9>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, num_epochs, train_loader, test_loader, optimizer, loss_function, alpha)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0ml1_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ml1_norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m# Calculate the gradients with help of back propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m            \u001b[1;31m#print('W')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m            \u001b[1;31m#print(model.GCNLayer1.W)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_test_acc = []\n",
    "\n",
    "t1 = time.time()\n",
    "for feature in ['de', 'psd', 'dasm', 'rasm', 'dcau', 'asm']:\n",
    "    \n",
    "    final_test_feature_acc = []\n",
    "    signals = feature\n",
    "    \n",
    "    for freq in range(4):\n",
    "        \n",
    "        glob_test_acc = []\n",
    "        use_freq = freq\n",
    "\n",
    "        for sub in range(1, 16):\n",
    "            subject_number = sub\n",
    "            \n",
    "            \n",
    "            for exp in range(3):\n",
    "                \n",
    "                \n",
    "                experiment_number = exp\n",
    "                dataset = SignalDataset(path, signals, subject_number, experiment_number, \n",
    "                                        labels, smoothing_method, freq = use_freq)\n",
    "                train_dataset, test_dataset = random_split(dataset, [9, 6])\n",
    "                batch_size = 1\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "                test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
    "                #matrix we will work with\n",
    "                W = choose_W(feature, W_de, W_dasm, W_dcau, W_asm)\n",
    "                W_ = W[use_freq].type(torch.float32).clone().detach().requires_grad_(True) #work with the first brain frequency\n",
    "\n",
    "                #model\n",
    "                model = feature_model_adaptator(feature, device)\n",
    "\n",
    "                #oss functions and optimizer\n",
    "\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9) \n",
    "\n",
    "                train_loss, valid_loss, train_accuracy, valid_accuracy = train_model(model, num_epochs, \n",
    "                                                                        train_loader, test_loader, optimizer, \n",
    "                                                                                 criterion, alpha)\n",
    "\n",
    "                glob_test_acc.append(valid_accuracy)\n",
    "                \n",
    "        final_test_feature_acc.append(glob_test_acc)\n",
    "        \n",
    "    final_test_acc.append(final_test_feature_acc)\n",
    "        \n",
    "t2 = time.time()\n",
    "print(t1-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8ae4d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "45\n",
      "45\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "useful_data = final_test_feature_acc\n",
    "print(len(final_test_feature_acc))\n",
    "print(len(final_test_feature_acc[0]))\n",
    "print(len(final_test_feature_acc[1]))\n",
    "print(len(glob_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce74c9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Best validation accuracy got -----------\n",
      "The best validation accuracy got is 83.33333333333334 for the subject 2 and the experiment 1 at the epoch 110\n",
      "\n",
      "------ Best validation accuracy got for each experiment ----------\n",
      "                                     \n",
      "subject: 1, experiment: 1   33.333333\n",
      "subject: 1, experiment: 2   33.333333\n",
      "subject: 1, experiment: 3   50.000000\n",
      "subject: 2, experiment: 1   83.333333\n",
      "subject: 2, experiment: 2   16.666667\n",
      "subject: 2, experiment: 3   50.000000\n",
      "subject: 3, experiment: 1   50.000000\n",
      "subject: 3, experiment: 2   33.333333\n",
      "subject: 3, experiment: 3   66.666667\n",
      "subject: 4, experiment: 1   33.333333\n",
      "subject: 4, experiment: 2   66.666667\n",
      "subject: 4, experiment: 3   50.000000\n",
      "subject: 5, experiment: 1   50.000000\n",
      "subject: 5, experiment: 2   33.333333\n",
      "subject: 5, experiment: 3   50.000000\n",
      "subject: 6, experiment: 1   33.333333\n",
      "subject: 6, experiment: 2   16.666667\n",
      "subject: 6, experiment: 3   33.333333\n",
      "subject: 7, experiment: 1   66.666667\n",
      "subject: 7, experiment: 2   50.000000\n",
      "subject: 7, experiment: 3   33.333333\n",
      "subject: 8, experiment: 1   66.666667\n",
      "subject: 8, experiment: 2   16.666667\n",
      "subject: 8, experiment: 3   50.000000\n",
      "subject: 9, experiment: 1   66.666667\n",
      "subject: 9, experiment: 2   50.000000\n",
      "subject: 9, experiment: 3   33.333333\n",
      "subject: 10, experiment: 1  50.000000\n",
      "subject: 10, experiment: 2  33.333333\n",
      "subject: 10, experiment: 3  50.000000\n",
      "subject: 11, experiment: 1  66.666667\n",
      "subject: 11, experiment: 2  33.333333\n",
      "subject: 11, experiment: 3  66.666667\n",
      "subject: 12, experiment: 1  66.666667\n",
      "subject: 12, experiment: 2  33.333333\n",
      "subject: 12, experiment: 3  33.333333\n",
      "subject: 13, experiment: 1  50.000000\n",
      "subject: 13, experiment: 2  33.333333\n",
      "subject: 13, experiment: 3  83.333333\n",
      "subject: 14, experiment: 1  33.333333\n",
      "subject: 14, experiment: 2  33.333333\n",
      "subject: 14, experiment: 3  16.666667\n",
      "subject: 15, experiment: 1  50.000000\n",
      "subject: 15, experiment: 2  33.333333\n",
      "subject: 15, experiment: 3  50.000000\n",
      "\n",
      "------ Mean and std of the validation accuracy -----------\n",
      "23.703703703703706\n",
      "16.6625509321511\n"
     ]
    }
   ],
   "source": [
    "test_acc_delta = np.array(final_test_feature_acc[0])\n",
    "print('------- Best validation accuracy got -----------')\n",
    "best_val_acc_value = np.max(test_acc_delta[0])\n",
    "best_val_acc_epoch = np.argmax(test_acc_delta[0])\n",
    "best_val_acc_subexp = 0\n",
    "for k in range(1, test_acc_delta.shape[0]):\n",
    "    best_val_acc_value_aux = np.max(test_acc_delta[k])\n",
    "    if best_val_acc_value_aux > best_val_acc_value :\n",
    "        best_val_acc_value = best_val_acc_value_aux\n",
    "        best_val_acc_epoch = np.argmax(test_acc_delta[k])\n",
    "        best_val_acc_subexp = k\n",
    "print('The best validation accuracy got is {} for the subject {} and the experiment {} at the epoch {}'.format(best_val_acc_value, (best_val_acc_subexp  // 3) + 1, (best_val_acc_subexp  % 3) + 1, best_val_acc_epoch ))\n",
    "print('')\n",
    "\n",
    "print('------ Best validation accuracy got for each experiment ----------')\n",
    "pd_idx = []\n",
    "for sub in range(1, 16):\n",
    "    for exp in range(1, 4):\n",
    "        pd_idx.append('subject: ' + str(sub) + ', experiment: ' + str(exp))\n",
    "df_m = pd.DataFrame(np.max(test_acc_delta, axis = 1), index = pd_idx, columns = [''])\n",
    "print(df_m)\n",
    "print('')\n",
    "\n",
    "print('------ Mean and std of the validation accuracy -----------')\n",
    "print(np.mean(test_acc_delta[:, -1]))\n",
    "print(np.std(test_acc_delta[:, -1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f8b969b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33.33333333, 16.66666667,  0.        , 66.66666667,  0.        ,\n",
       "       33.33333333, 33.33333333,  0.        , 50.        ,  0.        ,\n",
       "       33.33333333, 33.33333333, 50.        ,  0.        , 16.66666667,\n",
       "       16.66666667,  0.        , 50.        ,  0.        , 33.33333333,\n",
       "        0.        ,  0.        , 33.33333333, 16.66666667, 50.        ,\n",
       "       33.33333333, 33.33333333, 16.66666667, 33.33333333, 16.66666667,\n",
       "        0.        , 33.33333333, 33.33333333, 16.66666667, 33.33333333,\n",
       "       33.33333333, 33.33333333,  0.        , 50.        , 16.66666667,\n",
       "       33.33333333, 16.66666667,  0.        , 33.33333333,  0.        ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc_theta[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71f53298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Best validation accuracy got -----------\n",
      "The best validation accuracy got is 83.33333333333334 for the subject 13 and the experiment 3 at the epoch 56\n",
      "\n",
      "------ Best validation accuracy got for each experiment ----------\n",
      "                                     \n",
      "subject: 1, experiment: 1   50.000000\n",
      "subject: 1, experiment: 2   33.333333\n",
      "subject: 1, experiment: 3   16.666667\n",
      "subject: 2, experiment: 1   66.666667\n",
      "subject: 2, experiment: 2   33.333333\n",
      "subject: 2, experiment: 3   33.333333\n",
      "subject: 3, experiment: 1   33.333333\n",
      "subject: 3, experiment: 2   16.666667\n",
      "subject: 3, experiment: 3   66.666667\n",
      "subject: 4, experiment: 1   33.333333\n",
      "subject: 4, experiment: 2   50.000000\n",
      "subject: 4, experiment: 3   33.333333\n",
      "subject: 5, experiment: 1   50.000000\n",
      "subject: 5, experiment: 2    0.000000\n",
      "subject: 5, experiment: 3   33.333333\n",
      "subject: 6, experiment: 1   66.666667\n",
      "subject: 6, experiment: 2   16.666667\n",
      "subject: 6, experiment: 3   50.000000\n",
      "subject: 7, experiment: 1   33.333333\n",
      "subject: 7, experiment: 2   66.666667\n",
      "subject: 7, experiment: 3   33.333333\n",
      "subject: 8, experiment: 1   66.666667\n",
      "subject: 8, experiment: 2   33.333333\n",
      "subject: 8, experiment: 3   50.000000\n",
      "subject: 9, experiment: 1   50.000000\n",
      "subject: 9, experiment: 2   50.000000\n",
      "subject: 9, experiment: 3   33.333333\n",
      "subject: 10, experiment: 1  50.000000\n",
      "subject: 10, experiment: 2  33.333333\n",
      "subject: 10, experiment: 3  33.333333\n",
      "subject: 11, experiment: 1  50.000000\n",
      "subject: 11, experiment: 2  33.333333\n",
      "subject: 11, experiment: 3  66.666667\n",
      "subject: 12, experiment: 1  66.666667\n",
      "subject: 12, experiment: 2  33.333333\n",
      "subject: 12, experiment: 3  50.000000\n",
      "subject: 13, experiment: 1  33.333333\n",
      "subject: 13, experiment: 2  50.000000\n",
      "subject: 13, experiment: 3  83.333333\n",
      "subject: 14, experiment: 1  33.333333\n",
      "subject: 14, experiment: 2  50.000000\n",
      "subject: 14, experiment: 3  50.000000\n",
      "subject: 15, experiment: 1  50.000000\n",
      "subject: 15, experiment: 2  50.000000\n",
      "subject: 15, experiment: 3  33.333333\n",
      "\n",
      "------ Mean and std of the validation accuracy -----------\n",
      "22.962962962962955\n",
      "18.015387417800458\n"
     ]
    }
   ],
   "source": [
    "test_acc_theta = np.array(final_test_feature_acc[1])\n",
    "print('------- Best validation accuracy got -----------')\n",
    "best_val_acc_value = np.max(test_acc_theta[0])\n",
    "best_val_acc_epoch = np.argmax(test_acc_theta[0])\n",
    "best_val_acc_subexp = 0\n",
    "for k in range(1, test_acc_delta.shape[0]):\n",
    "    best_val_acc_value_aux = np.max(test_acc_theta[k])\n",
    "    if best_val_acc_value_aux > best_val_acc_value :\n",
    "        best_val_acc_value = best_val_acc_value_aux\n",
    "        best_val_acc_epoch = np.argmax(test_acc_theta[k])\n",
    "        best_val_acc_subexp = k\n",
    "print('The best validation accuracy got is {} for the subject {} and the experiment {} at the epoch {}'.format(best_val_acc_value, (best_val_acc_subexp  // 3) + 1, (best_val_acc_subexp  % 3) + 1, best_val_acc_epoch ))\n",
    "print('')\n",
    "\n",
    "print('------ Best validation accuracy got for each experiment ----------')\n",
    "pd_idx = []\n",
    "for sub in range(1, 16):\n",
    "    for exp in range(1, 4):\n",
    "        pd_idx.append('subject: ' + str(sub) + ', experiment: ' + str(exp))\n",
    "df_m = pd.DataFrame(np.max(test_acc_theta, axis = 1), index = pd_idx, columns = [''])\n",
    "print(df_m)\n",
    "print('')\n",
    "\n",
    "print('------ Mean and std of the validation accuracy -----------')\n",
    "print(np.mean(test_acc_theta[:, -1]))\n",
    "print(np.std(test_acc_theta[:, -1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97d65c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Best validation accuracy got -----------\n",
      "The best validation accuracy got is 83.33333333333334 for the subject 1 and the experiment 1 at the epoch 90\n",
      "\n",
      "------ Best validation accuracy got for each experiment ----------\n",
      "                                    \n",
      "subject: 1, experiment: 1  83.333333\n",
      "subject: 1, experiment: 2  16.666667\n",
      "subject: 1, experiment: 3  33.333333\n",
      "subject: 2, experiment: 1  16.666667\n",
      "subject: 2, experiment: 2  33.333333\n",
      "subject: 2, experiment: 3  50.000000\n",
      "subject: 3, experiment: 1  33.333333\n",
      "subject: 3, experiment: 2  50.000000\n",
      "subject: 3, experiment: 3  33.333333\n",
      "subject: 4, experiment: 1  33.333333\n",
      "subject: 4, experiment: 2  33.333333\n",
      "subject: 4, experiment: 3  33.333333\n",
      "subject: 5, experiment: 1  83.333333\n",
      "\n",
      "------ Mean and std of the validation accuracy -----------\n",
      "28.2051282051282\n",
      "23.91635654638158\n"
     ]
    }
   ],
   "source": [
    "test_acc_alpha = np.array(glob_test_acc)\n",
    "print('------- Best validation accuracy got -----------')\n",
    "best_val_acc_value = np.max(test_acc_alpha[0])\n",
    "best_val_acc_epoch = np.argmax(test_acc_alpha[0])\n",
    "best_val_acc_subexp = 0\n",
    "for k in range(1, test_acc_alpha.shape[0]):\n",
    "    best_val_acc_value_aux = np.max(test_acc_alpha[k])\n",
    "    if best_val_acc_value_aux > best_val_acc_value :\n",
    "        best_val_acc_value = best_val_acc_value_aux\n",
    "        best_val_acc_epoch = np.argmax(test_acc_alpha[k])\n",
    "        best_val_acc_subexp = k\n",
    "print('The best validation accuracy got is {} for the subject {} and the experiment {} at the epoch {}'.format(best_val_acc_value, (best_val_acc_subexp  // 3) + 1, (best_val_acc_subexp  % 3) + 1, best_val_acc_epoch ))\n",
    "print('')\n",
    "\n",
    "print('------ Best validation accuracy got for each experiment ----------')\n",
    "pd_idx = []\n",
    "for sub in range(1, 16):\n",
    "    for exp in range(1, 4):\n",
    "        pd_idx.append('subject: ' + str(sub) + ', experiment: ' + str(exp))\n",
    "df_m = pd.DataFrame(np.max(test_acc_alpha, axis = 1), index = pd_idx[:13], columns = [''])\n",
    "print(df_m)\n",
    "print('')\n",
    "\n",
    "print('------ Mean and std of the validation accuracy -----------')\n",
    "print(np.mean(test_acc_alpha[:, -1]))\n",
    "print(np.std(test_acc_alpha[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c562e522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66.66666667, 16.66666667,  0.        ,  0.        , 33.33333333,\n",
       "       16.66666667, 33.33333333, 16.66666667, 33.33333333, 33.33333333,\n",
       "       33.33333333,  0.        , 83.33333333])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc_alpha[:, -1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
