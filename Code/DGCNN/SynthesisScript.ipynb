{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db602fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b456d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe9d6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions use inside the model\n",
    "\n",
    "##### CREATE A CLASS IF EVERYTHING IS FINISHED ####\n",
    "#more elegant\n",
    "\n",
    "def D_matrix(W, multifreq = True):\n",
    "    if multifreq:\n",
    "        for freq in range(W.shape[0]):\n",
    "            D[freq, :, :] = torch.diag(W[freq].sum(dim=-1))\n",
    "    else :\n",
    "        D = torch.diag(W.sum(dim=-1))\n",
    "    return D\n",
    "\n",
    "def laplacian(W, multifreq = True): #here we calculate the non noramlized Laplacian matrix of the graph\n",
    "    #multifreq means if we get the five frequencies in one or not\n",
    "    L = D_matrix(W, multifreq = multifreq) - W\n",
    "    L = L.type(torch.float32)\n",
    "    return L\n",
    "\n",
    "def laplacian_norm_mod(W, multifreq = False): #we don't include multifreq option for this function\n",
    "    D_square = D_square_matrix(W, multifreq = multifreq)\n",
    "    L_norm_mod = torch.matmul(D_square, torch.matmul(W + torch.eye(W.shape[0]), D_square))\n",
    "    return L_norm_mod\n",
    "\n",
    "\n",
    "def x_shaper(x):\n",
    "    \"\"\"\n",
    "    Arg: x as when it outcome from the dataset\n",
    "    return: x as we wants tensor with (freq, nb_channel, activity)\n",
    "    \"\"\"\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    return torch.from_numpy(x)\n",
    "\n",
    "\n",
    "def cheby_laplacian(L, x, theta, multifreq = True):\n",
    "    \"\"\"\n",
    "    Arg: Laplacian matrix, x the extracted signal, theta chebyshev coefficients, K the order of the polynom\n",
    "    \n",
    "    we suppose that x was transform to a tensor and that its shape was made as (freq, nb channel, activity)\n",
    "    \n",
    "    Return: the sum thetak*Tk(L)*x\n",
    "    \"\"\"\n",
    " \n",
    "    y = torch.zeros_like(x) \n",
    "    K = theta.shape[0]\n",
    "    \n",
    "    if multifreq :\n",
    "        L_tilde = torch.zeros_like(L) \n",
    "        for freq in range(L.shape[0]):\n",
    "            #chebyshev polynoms is a basis in the domain [-1, 1]\n",
    "            with torch.no_grad():\n",
    "                eigenvalues, eigenvectors = torch.linalg.eig(L[freq, :, :])\n",
    "                eigenvalues = eigenvalues.real #since L is symmetric real L is diagonalizable in the real space\n",
    "                max_lambda = torch.max(eigenvalues).item() \n",
    "            L_tilde[freq, :, :] = ((2*L[freq, :, :]) / max_lambda) - torch.eye(L.shape[1])\n",
    "            \n",
    "            x_ = x[freq, :, :]\n",
    "            L_ = L_tilde[freq, :, :]\n",
    "            L_.type(torch.float32)\n",
    "\n",
    "            x0_hat = x_\n",
    "            x1_hat = torch.matmul(L_, x_)\n",
    "\n",
    "            if K == 0:\n",
    "                y[freq, :, :] = x0_hat*theta[0]\n",
    "            if K == 1:\n",
    "                y[freq, :, :] = x1_hat*theta[1] + x0_hat*theta[0]\n",
    "            else:\n",
    "                y[freq, :, :] = x0_hat*theta[0] + x1_hat*theta[1] \n",
    "                for k in range(2, K):\n",
    "                    x2_hat = 2*torch.matmul(L_, x1_hat) - x0_hat\n",
    "                    y[freq, :, :] += x2_hat*theta[k]\n",
    "                    x1_hat = x2_hat\n",
    "                    x0_hat = x1_hat\n",
    "    else :\n",
    "        with torch.no_grad():\n",
    "            eigenvalues, eigenvectors = torch.linalg.eig(L)\n",
    "            eigenvalues = eigenvalues.real #since L is symmetric real L is diagonalizable in the real space\n",
    "            max_lambda = torch.max(eigenvalues).item() \n",
    "            \n",
    "        #rescaling\n",
    "        L_tilde = ((2*L) / max_lambda) - torch.eye(L.shape[0])\n",
    "        \n",
    "        #intialization of variables\n",
    "        x_ = x\n",
    "        L_ = L_tilde\n",
    "        \n",
    "        L_ = L_.type(torch.float32)\n",
    "\n",
    "        x0_hat = x_\n",
    "        x1_hat = torch.matmul(L_, x_)\n",
    "        \n",
    "        #calculus of Chebyshev polynoms\n",
    "        if K == 0:\n",
    "            y = x0_hat*theta[0]\n",
    "        if K == 1:\n",
    "            y = x1_hat*theta[1] + x0_hat*theta[0]\n",
    "        else:\n",
    "            y = x0_hat*theta[0] + x1_hat*theta[1] \n",
    "            for k in range(2, K):\n",
    "                x2_hat = 2*torch.matmul(L_, x1_hat) - x0_hat\n",
    "                y += x2_hat*theta[k]\n",
    "                x1_hat = x2_hat\n",
    "                x0_hat = x1_hat              \n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def polynomial_approximation(L, x, theta, multifreq = True):\n",
    "    \"\"\"\n",
    "    Arg: Laplacian matrix, x the extracted signal, theta chebyshev coefficients, K the order of the polynom\n",
    "    \n",
    "    we suppose that x was transform to a tensor and that its shape was made as (freq, nb channel, activity)\n",
    "    \n",
    "    Return: the U*sum theta[k]*lambda**k*U^{T}\n",
    "    \"\"\"\n",
    " \n",
    "    y = torch.zeros_like(x) \n",
    "    K = theta.shape[0]\n",
    "    \n",
    "    if multifreq :\n",
    "        L_tilde = torch.zeros_like(L) \n",
    "        for freq in range(L.shape[0]):\n",
    "            with torch.no_grad():\n",
    "                eigenvalues, eigenvectors = torch.linalg.eig(L[freq, :, :])\n",
    "                eigenvalues = eigenvalues.real #since L is symmetric real L is diagonalizable in the real space\n",
    "                U = eigenvectors.real\n",
    "        \n",
    "            eigen_values_diag = torch.diag(eigenvalues)\n",
    "            aux = torch.zeros_like(eigen_values_diag)\n",
    "            for k in range(K):\n",
    "                aux += torch.linalg.matrix_power(eigen_values_diag, k)*theta[k]\n",
    "            y[freq, :, :] = torch.matmul(torch.matmul(U, torch.matmul(aux, U.transpose(1, 0))), x)\n",
    "            \n",
    "    else :  \n",
    "        #since we don't work with chebyshev approximation there is no need to rescale the Laplacian\n",
    "        with torch.no_grad():\n",
    "                eigenvalues, eigenvectors = torch.linalg.eig(L)\n",
    "                eigenvalues = eigenvalues.real #since L is symmetric real L is diagonalizable in the real space\n",
    "                U = eigenvectors.real\n",
    "        \n",
    "        eigen_values_diag = torch.diag(eigenvalues)\n",
    "        aux = torch.zeros_like(eigen_values_diag)\n",
    "        for k in range(K):   \n",
    "            aux += torch.linalg.matrix_power(eigen_values_diag, k)*theta[k]\n",
    "        y = torch.matmul(torch.matmul(U, torch.matmul(aux, U.transpose(1, 0))), x) \n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, num_epochs, train_loader, test_loader, optimizer, loss_function, alpha):\n",
    "\n",
    "    #training of the model\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        ############################\n",
    "        # Train\n",
    "        ############################\n",
    "\n",
    "        iter_loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.train()        # Put the network into training mode\n",
    "\n",
    "        for i, (signal, label_) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "            outputs = model(signal)      # Do the forward pass\n",
    "            loss = loss_function(outputs, label_) # Calculate the loss\n",
    "            iter_loss += loss.item() # Accumulate the loss\n",
    "\n",
    "            #L1-Regularization\n",
    "            l1_norm = sum(abs(p).sum() for p in model.parameters())\n",
    "            loss = loss + alpha*l1_norm\n",
    "            loss.backward()           # Calculate the gradients with help of back propagation\n",
    "           #print('W')\n",
    "           #print(model.GCNLayer1.W)\n",
    "           #print(model.GCNLayer1.W.grad)\n",
    "           #print('theta')\n",
    "           #print(model.GCNLayer1.theta)\n",
    "           #print(model.GCNLayer1.theta.grad)\n",
    "            optimizer.step()          # Adjust the parameters based on the gradients\n",
    "\n",
    "\n",
    "            # Record the correct predictions for training data \n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            correct += (predicted == label_).sum()\n",
    "            iterations += 1\n",
    "\n",
    "        # Record the training loss\n",
    "        train_loss.append(iter_loss / iterations)\n",
    "        # Record the training accuracy\n",
    "        train_accuracy.append((100 * correct / float(len(train_dataset))))\n",
    "\n",
    "\n",
    "        ############################\n",
    "        # Test\n",
    "        ############################\n",
    "\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.eval()                    # Put the network into evaluate mode\n",
    "\n",
    "        for i, (signal, label_) in enumerate(test_loader):\n",
    "\n",
    "            outputs = model(signal)      # Do the forward pass\n",
    "            loss += criterion(outputs, label_).item() # Calculate the loss\n",
    "\n",
    "            # Record the correct predictions for training data\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            correct += (predicted == label_).sum()\n",
    "\n",
    "            iterations += 1\n",
    "\n",
    "        # Record the validation loss\n",
    "        valid_loss.append(loss / iterations)\n",
    "        # Record the validation accuracy\n",
    "        correct_scalar = np.array([correct.clone()])[0]\n",
    "        valid_accuracy.append(correct_scalar.item() / len(test_dataset) * 100.0)\n",
    "        \n",
    "    return train_loss, valid_loss, train_accuracy, valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb482cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DGCNLayerCheb(nn.Module):\n",
    "    def __init__(self, W, K):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(W)\n",
    "        self.theta = nn.Parameter(torch.FloatTensor(K, 1))\n",
    "        self.theta = nn.init.uniform_(self.theta, 0, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        L = laplacian(self.W, multifreq = False)\n",
    "        X = cheby_laplacian(L, x, self.theta, multifreq = False)\n",
    "        return X\n",
    "    \n",
    "class DGCN_Cheb(nn.Module):\n",
    "    def __init__(self, W, K, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.GCNLayer1 = DGCNLayerCheb(W, K)\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.GCNLayer1(x) \n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.l1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l2(x)\n",
    "        return self.tanh(x)\n",
    "    \n",
    "\n",
    "class DGCNLayerPoly(nn.Module):\n",
    "    def __init__(self, W, K):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(W)\n",
    "        self.theta = nn.Parameter(torch.FloatTensor(K, 1))\n",
    "        self.theta = nn.init.uniform_(self.theta, 0, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        L = laplacian(self.W, multifreq = False)\n",
    "        X = polynomial_approximation(L, x, self.theta, multifreq = False)\n",
    "        return X\n",
    "    \n",
    "class DGCN_Poly(nn.Module):\n",
    "    def __init__(self, W, K, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.DGCNLayer1 = DGCNLayerPoly(W, K)\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.DGCNLayer1(x) \n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.l1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l2(x)\n",
    "        return self.tanh(x)\n",
    "    \n",
    "    \n",
    "class DGCNLayerPolyNormMod(nn.Module):\n",
    "    def __init__(self, W, K):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(W)\n",
    "        self.theta = nn.Parameter(torch.FloatTensor(K, 1))\n",
    "        self.theta = nn.init.uniform_(self.theta, 0, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        L = laplacian_norm_mod(self.W, multifreq = False)\n",
    "        X = polynomial_approximation(L, x, self.theta, multifreq = False)\n",
    "        return X\n",
    "   \n",
    "#ssks\n",
    "class DGCN_PolyNormMod(nn.Module):\n",
    "    def __init__(self, W, K, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.GCNLayer1 = DGCNLayerPolyNormMod(W, K)\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.GCNLayer1(x) \n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.l1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l2(x)\n",
    "        return self.tanh(x)\n",
    "    \n",
    "class DGCNLayerChebNormMod(nn.Module):\n",
    "    def __init__(self, W, K):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(W)\n",
    "        self.theta = nn.Parameter(torch.FloatTensor(K, 1))\n",
    "        self.theta = nn.init.uniform_(self.theta, 0, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        L = laplacian_norm_mod(self.W, multifreq = False)\n",
    "        X = cheby_laplacian(L, x, self.theta, multifreq = False)\n",
    "        return X\n",
    "    \n",
    "class DGCN_ChebNormMod(nn.Module):\n",
    "    def __init__(self, W, K, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.GCNLayer1 = DGCNLayerChebNormMod(W, K)\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.GCNLayer1(x) \n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.l1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l2(x)\n",
    "        return self.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8db3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful functions\n",
    "\n",
    "def import_matfiles(path):\n",
    "    list_mat_ex = os.listdir(path)\n",
    "    #list_mat_file contains all interessant files for our research\n",
    "    list_mat_ex.remove('readme.txt')\n",
    "    #list_mat_file contains all interessant files for our research\n",
    "    label = list_mat_ex[-1]\n",
    "    trash = list_mat_ex.pop(-1)\n",
    "    return list_mat_ex, label\n",
    "    \n",
    "def find_max_shape(datas, freq = None):\n",
    "    #shape of data in datas (freq, nb channel, activity)\n",
    "    if freq == None:\n",
    "        max_ = datas[0].shape[2]\n",
    "        for data in datas[1:]:\n",
    "            act_shape = data.shape[2]\n",
    "            if act_shape > max_ :\n",
    "                max_ = act_shape\n",
    "    else :\n",
    "        #hape of data in datas (nb channel, activity)\n",
    "        max_ = datas[0].shape[1]\n",
    "        for data in datas[1:]:\n",
    "            act_shape = data.shape[1]\n",
    "            if act_shape > max_ :\n",
    "                max_ = act_shape\n",
    "    return max_  \n",
    "\n",
    "def x_shaper(x):\n",
    "    \"\"\"\n",
    "    Arg: x as when it outcome from the dataset\n",
    "    return: x as we wants tensor with (freq, nb_channel, activity)\n",
    "    \"\"\"\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    return torch.from_numpy(x).type(torch.float32)\n",
    "\n",
    "def datas_shaper(datas):\n",
    "    res = []\n",
    "    for data in datas:\n",
    "        print(data.shape)\n",
    "        res.append(x_shaper(data))\n",
    "    return res \n",
    "\n",
    "def padd(datas, freq = None):\n",
    "    #shape of data in datas (freq, nb channel activity)\n",
    "    max_shape = find_max_shape(datas, freq =freq)\n",
    "    res = []\n",
    "    if freq == None :\n",
    "        for data in datas:\n",
    "            aux = torch.zeros((data.shape[0], data.shape[1], max_shape))\n",
    "            if data.shape[2] < max_shape:\n",
    "                add_shape = max_shape - data.shape[2]\n",
    "                for freq in range(data.shape[0]):\n",
    "                    pad_tensor = torch.zeros((data.shape[1], add_shape))\n",
    "                    aux[freq, :, :] = torch.cat((data[freq, :, :], pad_tensor), dim=1)\n",
    "                    aux = aux.type(torch.float32)\n",
    "                res.append(aux)\n",
    "            else :\n",
    "                res.append(data.type(torch.float32))\n",
    "    else :\n",
    "        for data in datas:\n",
    "            aux = torch.zeros((data.shape[0], max_shape))\n",
    "            if data.shape[1] < max_shape:\n",
    "                add_shape = max_shape - data.shape[1]\n",
    "                pad_tensor = torch.zeros((data.shape[0], add_shape))\n",
    "                aux = torch.cat((data, pad_tensor), dim=1)\n",
    "                aux = aux.type(torch.float32)\n",
    "                res.append(aux)\n",
    "            else :\n",
    "                res.append(data.type(torch.float32))\n",
    "    return res\n",
    "\n",
    "def normalize(x, freq = None):\n",
    "    if freq == None:\n",
    "        for frequencies in range(x.shape[0]):\n",
    "            mean_ = torch.mean(x[frequencies, :, :], 1).reshape(-1, 1)\n",
    "            std_ = torch.std(x[frequencies, : ,: ], 1).reshape(-1, 1)\n",
    "            x[frequencies, :, :] = (x[frequencies, :, :] - mean_) / std_\n",
    "            return x\n",
    "    else :\n",
    "        mean_ = torch.mean(x[freq, :, :], 1).reshape(-1, 1)\n",
    "        std_ = torch.std(x[freq, : ,: ], 1).reshape(-1, 1)\n",
    "        x[freq, :, :] = (x[freq, :, :] - mean_) / std_\n",
    "        return x[freq, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc6be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version to improve in a near future..\n",
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, path, signals, subject_number, experiment_number, labels, smoothing_method, freq = None):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            path: path to folder with all the .mat files of the dataset\n",
    "            signals: feature that we want to extract\n",
    "            subject_number: subject of the experimenr\n",
    "            epxeriment_number: number of the experiment, like 0 week 1 week or 2 week \n",
    "            (MAYBE I MISUNDERSTOOD THIS PART READ AGAIN DATASET DETAILS)\n",
    "            labels: labels list\n",
    "            smoothing method: movingAve or LDS\n",
    "            freq: 0, 1, 2, 3 or 4 for just specific one and None if you want them all\n",
    "        \"\"\"\n",
    "        list_file, _ = import_matfiles(path)\n",
    "        #selection of interesting files that means the one related to the subject\n",
    "        list_sub_file = []\n",
    "        for file in list_file:\n",
    "            num = file.split('_')[0]\n",
    "            if int(num) == subject_number: \n",
    "                list_sub_file.append(file)\n",
    "             \n",
    "        working_file = list_sub_file[experiment_number]\n",
    "        \n",
    "        dic = loadmat(path + '\\\\' + working_file) \n",
    "        if smoothing_method == 'movingAve':\n",
    "            smooth = signals.lower() + '_movingAve'\n",
    "        elif smooting_method == 'LDS':\n",
    "            smooth = signals.lower() + '_LDS'\n",
    "        else :\n",
    "            raise ValueError('Please select a good smoothing method: movingAve or LDS')\n",
    "        \n",
    "        #maybe a list is not the best tool to store it\n",
    "        datas = []\n",
    "        for k in range(15):\n",
    "            sig = dic[smooth + str(k+1)]\n",
    "            sig = x_shaper(sig)\n",
    "            sig = normalize(sig, freq = freq) #change the freq if needed!!!!!!!!!!!!\n",
    "            datas.append(sig)\n",
    "            \n",
    "        \n",
    "        datas = padd(datas, freq = freq) #everything is happening in this function, the changement from Long to Float\n",
    "                                    #and the slicing for the frequency we want\n",
    "\n",
    "        labels = labels.reshape(-1, )\n",
    "        labels = labels + 1 #sliding to use CrossEntropy Pytorch function\n",
    "        labels = torch.from_numpy(labels)\n",
    "                                  \n",
    "                                  \n",
    "        self.datas = datas\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.datas[idx], self.labels[idx].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4b304",
   "metadata": {},
   "source": [
    "## Creation of adjency matrices for every features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c429d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#channels according to the index in the dataset\n",
    "channel = ['FP1', 'FPZ', 'FP2', 'AF3', 'AF4', 'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8', \n",
    "                'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6','FT8','T7','C5','C3','C1',\n",
    "                'CZ','C2','C4','C6', 'T8','TP7','CP5','CP3','CP1','CPZ','CP2','CP4','CP6','TP8','P7',\n",
    "                'P5','P3','P1','PZ','P2','P4','P6','P8','PO7','PO5','PO3','POZ','PO4','PO6','PO8','CB1',\n",
    "                'O1','OZ','O2','CB2']\n",
    "idx = np.arange(62)\n",
    "df_channel = pd.DataFrame({'idx': idx, 'channel': channel })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bef62b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 62, 62])\n"
     ]
    }
   ],
   "source": [
    "#initialization of the adjency matrix for DE and PSD features\n",
    "feature_number = 62\n",
    "W = np.zeros((5, feature_number, feature_number))\n",
    "for freq in range(5):\n",
    "    #I don't see other way than initialiazing W manually\n",
    "    #was done accodingly to the EEG channel map and order, hope to don't have make mistake\n",
    "    #FP1\n",
    "    W[freq, 0, 1] = 1\n",
    "    W[freq, 0, 3] = 1\n",
    "    #FPZ\n",
    "    W[freq, 1, 0] = 1\n",
    "    W[freq, 1, 2] = 1\n",
    "    #FP2\n",
    "    W[freq, 2, 1] = 1\n",
    "    W[freq, 2, 4] = 1\n",
    "    #AF3\n",
    "    W[freq, 3, 8] = 1\n",
    "    W[freq, 3, 7] = 1\n",
    "    W[freq, 3, 6] = 1\n",
    "    W[freq, 3, 0] = 1\n",
    "    #AF4\n",
    "    W[freq, 4, 2] = 1\n",
    "    W[freq, 4, 10] = 1\n",
    "    W[freq, 4, 11] = 1\n",
    "    W[freq, 4, 12] = 1\n",
    "    #F7\n",
    "    W[freq, 5, 6] = 1\n",
    "    W[freq, 5, 14] = 1\n",
    "    #F5\n",
    "    W[freq, 6, 3] = 1\n",
    "    W[freq, 6, 5] = 1\n",
    "    W[freq, 6, 15] = 1\n",
    "    W[freq, 6, 7] = 1\n",
    "    #F3\n",
    "    W[freq, 7, 3] = 1\n",
    "    W[freq, 7, 8] = 1\n",
    "    W[freq, 7, 16] = 1\n",
    "    #F1\n",
    "    W[freq, 8, 9] = 1\n",
    "    W[freq, 8, 17] = 1\n",
    "    #FZ\n",
    "    W[freq, 9, 10] = 1\n",
    "    W[freq, 9, 18] = 1\n",
    "    #F2\n",
    "    W[freq, 10, 11] = 1\n",
    "    W[freq, 10, 19] = 1\n",
    "    #F4\n",
    "    W[freq, 11, 12] = 1\n",
    "    W[freq, 11, 20] = 1\n",
    "    #F6 \n",
    "    W[freq, 12, 13] = 1\n",
    "    W[freq, 12, 21] = 1\n",
    "    #F8\n",
    "    W[freq, 13, 22] = 1\n",
    "    #FT7\n",
    "    W[freq, 14, 15] = 1\n",
    "    W[freq, 14, 23] = 1\n",
    "    #FC5 to TP8\n",
    "    for k in range(15, 41): #till TP8 (number 40) we have two new neigbors: on the right and under +1 and +9 (nine electrodes per line)\n",
    "        if k == 22 or k == 31 or k == 40 :\n",
    "            W[freq, k, k+9] = 1\n",
    "        else:\n",
    "            W[freq, k, k+1] = 1\n",
    "            W[freq, k, k+9] = 1\n",
    "    #P7\n",
    "    W[freq, 41, 42] = 1\n",
    "    W[freq, 41, 50] = 1\n",
    "    #P5\n",
    "    W[freq, 42, 43] = 1\n",
    "    W[freq, 42, 51] = 1\n",
    "    #P3\n",
    "    W[freq, 43, 44] = 1\n",
    "    #P1\n",
    "    W[freq, 44, 45] = 1\n",
    "    W[freq, 44, 52] = 1\n",
    "    #PZ\n",
    "    W[freq, 45, 46] = 1\n",
    "    W[freq, 45, 53] = 1\n",
    "    #P2\n",
    "    W[freq, 46, 47] = 1\n",
    "    W[freq, 46, 54] = 1\n",
    "    #P4\n",
    "    W[freq, 47, 48] = 1\n",
    "    #P6\n",
    "    W[freq, 48, 49] = 1\n",
    "    W[freq, 48, 55] = 1\n",
    "    #P8\n",
    "    W[freq, 49, 56] = 1\n",
    "    #PO7\n",
    "    W[freq, 50, 51] = 1\n",
    "    W[freq, 50, 57] = 1\n",
    "    #PO5\n",
    "    W[freq, 51, 52] = 1\n",
    "    W[freq, 50, 57] = 1\n",
    "    #PO3\n",
    "    W[freq, 52, 53] = 1\n",
    "    W[freq, 52, 58] = 1\n",
    "    #POZ\n",
    "    W[freq, 53, 54] = 1\n",
    "    W[freq, 53, 59] = 1\n",
    "    #PO4\n",
    "    W[freq, 54, 55] = 1\n",
    "    W[freq, 54, 60] = 1\n",
    "    #PO6\n",
    "    W[freq, 55, 56] = 1\n",
    "    W[freq, 55, 61] = 1\n",
    "    #PO8\n",
    "    W[freq, 56, 61] = 1\n",
    "    #CB1\n",
    "    W[freq, 57, 58] = 1\n",
    "    #O1\n",
    "    W[freq, 58, 59] = 1\n",
    "    #OZ\n",
    "    W[freq, 59, 60] = 1\n",
    "    #O2\n",
    "    W[freq, 60, 61] = 1\n",
    "\n",
    "for freq in range(5):\n",
    "    for i in range(feature_number):\n",
    "        for j in range(i, feature_number):\n",
    "            if W[freq, j, i] == 0:\n",
    "                W[freq, j, i] = W[freq, i, j]\n",
    "    W[freq, :, :] = W[freq, :, :] - np.eye(feature_number)\n",
    "\n",
    "#for some training stability reasons we initialized the diagonal of W with 1\n",
    "for freq in range(5):\n",
    "    W[freq, :, :] = W[freq, :, :] + np.eye(W.shape[1], dtype=int)\n",
    "\n",
    "W_de = torch.from_numpy(W).type(torch.float32)\n",
    "print(W_de.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab654b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 27, 27])\n",
      "torch.Size([5, 23, 23])\n",
      "torch.Size([5, 54, 54])\n"
     ]
    }
   ],
   "source": [
    "#initialization for DASM and RASM \n",
    "#WE ARE GONNA TAKE A TRUNCATED FORM OF THE DE ADJENCY MATRIX \n",
    "#take the same pattern which is a repeated one \n",
    "\n",
    "W_dasm = torch.zeros(5, 27, 27)\n",
    "for freq in range(5):\n",
    "    W_dasm[freq, :, :] = W_de[0, :27, :27] #we take 0 because it's all the same for every frequencies\n",
    "print(W_dasm.shape)\n",
    "\n",
    "#initialization for DCAU\n",
    "W_dcau = torch.zeros(5, 23, 23)\n",
    "for freq in range(5):\n",
    "    W_dcau[freq, :, :] = W_de[0, :23, :23] #we take 0 because it's all the same for every frequencies\n",
    "print(W_dcau.shape)\n",
    "\n",
    "#intialization for ASM \n",
    "W_asm = torch.zeros(5, 54, 54)\n",
    "for freq in range(5):\n",
    "    W_asm[freq, :, :] = W_de[0, :54, :54] #we take 0 because it's all the same for every frequencies\n",
    "print(W_asm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0a79fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to loop over the entire dataset\n",
    "def get_accuracy_results(feature, W_feature, model_name):\n",
    "    feature_list = ['de', 'psd', 'dasm', 'rasm', 'dcau', 'asm']\n",
    "    if feature not in feature_list:\n",
    "        raise TypeError('Please select a feature in ' + str(feature_list))\n",
    "    \n",
    "    path = r'C:\\Users\\harol\\UsefulCode\\CP_DSAI\\Project\\Dataset\\SEED\\ExtractedFeatures'\n",
    "    signals = feature\n",
    "    \n",
    "    l, label_ = import_matfiles(path)\n",
    "    labels = loadmat(path + '\\\\' + label_)['label']\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    smoothing_method = 'movingAve'\n",
    "    batch_size = 1\n",
    "    learning_rate = 0.0001\n",
    "    alpha = 1e-7\n",
    "    K = 10\n",
    "    num_epochs = 1000\n",
    "\n",
    "    glob_test_acc = []\n",
    "\n",
    "    for freq in range(4): #5 frequencies used across the dataset\n",
    "        freq_test_acc = []\n",
    "        for sub in range(1, 16):#15 subject numbered form 1 to 15\n",
    "            for exp in range(3):   #3 experiment for each subject\n",
    "                dataset = SignalDataset(path, signals, sub, exp, \n",
    "                                        labels, smoothing_method, freq = freq)\n",
    "                train_dataset, test_dataset = random_split(dataset, [9, 6])\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, \n",
    "                                                           shuffle = True)\n",
    "                test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "                W_ = W_feature[freq].type(torch.float32).clone().detach().requires_grad_(True) \n",
    "\n",
    "                model = choose_model(model_name, device, W_, K, feature).to(device)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "\n",
    "                train_loss, valid_loss, train_accuracy, valid_accuracy = train_model(model, num_epochs, \n",
    "                                                                    train_loader, test_loader, optimizer, \n",
    "                                                                             criterion, alpha)\n",
    "                freq_test_acc.append(valid_accuracy)\n",
    "                \n",
    "                print('frequencies: {}, sub: {}, exp: {}'.format(freq, sub, exp))\n",
    "                print('Last Validation set accuracy score '+ str(valid_accuracy[-1]))\n",
    "\n",
    "        glob_test_acc.append(freq_test_acc)\n",
    "    \n",
    "    \n",
    "    return glob_test_acc\n",
    "\n",
    "def eh_merce_function(feature, glob_test_acc):\n",
    "    frequencies = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "    res= []\n",
    "    for freq in range(len(glob_test_acc)):\n",
    "        \n",
    "        aux = np.array(glob_test_acc[freq])\n",
    "        \n",
    "        \n",
    "        print('Feature: {}'.format(feature))\n",
    "        print('Frequency: ' + freq)\n",
    "        print('')\n",
    "        \n",
    "        print('------- Best validation accuracy got -----------')\n",
    "        best_val_acc_value = np.max(np.array(aux[0]))\n",
    "        best_val_acc_epoch = np.argmax(np.array(aux[0]))\n",
    "        best_val_acc_subexp = 0\n",
    "        for k in range(1, aux.shape[0]):\n",
    "            best_val_acc_value_aux = np.max(np.array(aux[k]))\n",
    "            if best_val_acc_value_aux > best_val_acc_value :\n",
    "                best_val_acc_value = best_val_acc_value_aux\n",
    "                best_val_acc_epoch = np.argmax(aux[k])\n",
    "                best_val_acc_subexp = k\n",
    "                \n",
    "        print('The best validation accuracy got is {} for the experiment {} of the subject {} at the epoch {}'.format(best_val_acc_value, (best_val_acc_subexp  % 3) + 1, (best_val_acc_subexp  // 3) + 1, best_val_acc_epoch ))\n",
    "        print('')\n",
    "\n",
    "        \n",
    "        print('------ Best validation accuracy got for each experiment ----------')\n",
    "        pd_idx = []\n",
    "        for sub in range(1, 16):\n",
    "            for exp in range(1, 4):\n",
    "                pd_idx.append('subject: ' + str(sub) + ', experiment: ' + str(exp))\n",
    "        df_m = pd.DataFrame(np.max(aux, axis = 1), index = pd_idx, columns = [''])\n",
    "        print(df_m)\n",
    "        print('')\n",
    "        \n",
    "        print('------ Mean of the validation accuracy for the last 50 epochs-----------')\n",
    "        df_mean = pd.DataFrame(np.mean(aux[:, 950:], axis = 1).reshape(-1, 1), \n",
    "                                               index = pd_idx, columns = [''])\n",
    "        print(df_mean)\n",
    "        \n",
    "        \n",
    "        \n",
    "        mean_ = np.mean(aux)\n",
    "        std_ = np.std(aux)\n",
    "        \n",
    "        res.append((mean_, std_))\n",
    "        print('')\n",
    "        print('----MEAN/STD-----')\n",
    "        print((mean_, std_))\n",
    "    return res\n",
    "\n",
    "#model = DGCN_Cheb(W_, K, 62*265, 50, 3).to(device)\n",
    "def choose_model(model_name, device, W, K, feature):\n",
    "    \n",
    "    feature_list = ['de', 'psd', 'dasm', 'rasm', 'dcau', 'asm']\n",
    "    if feature not in feature_list:\n",
    "        raise TypeError('Please select a feature in ' + str(feature_list))\n",
    "        \n",
    "    if (feature == 'de') or (feature == 'psd'):\n",
    "        if model_name == 'Chebyshev':\n",
    "            model = DGCN_Cheb(W, K, 62*265, 50, 3).to(device)\n",
    "        elif model_name == 'Poly':\n",
    "            model = DGCN_Poly(W, K, 62*265, 50, 3).to(device)\n",
    "        elif model_name == 'ChebyshevNormMod':\n",
    "            model = DGCN_ChebNormMod(W, K, 62*265, 50, 3).to(device)\n",
    "        elif model_name == 'ChebyshevGCN':\n",
    "            model = GCNCheb(W, K, 62*265, 50, 3).to(device)\n",
    "        else :\n",
    "            raise ValueError('Please select Cheby, Poly, ChebyshevNormMod, PolyNormMod')\n",
    "    \n",
    "    if (feature == 'dasm') or (feature == 'rasm'):\n",
    "        if model_name == 'Chebyshev':\n",
    "            model = DGCN_Cheb(W, K, 27*265, 50, 3).to(device)\n",
    "        elif model_name == 'Poly':\n",
    "            model = DGCN_Poly(W, K, 27*265, 50, 3).to(device)\n",
    "        elif model_name == 'ChebyshevNormMod':\n",
    "            model = DGCN_ChebNormMod(W, K, 27*265, 50, 3).to(device)\n",
    "        elif model_name == 'ChebyshevGCN':\n",
    "            model = GCNCheb(W, K, 27*265, 50, 3).to(device)\n",
    "        else :\n",
    "            raise ValueError('Please select Cheby, Poly, ChebyshevNormMod, PolyNormMod')\n",
    "            \n",
    "    if (feature == 'dcau'):\n",
    "        if model_name == 'Chebyshev':\n",
    "            model = DGCN_Cheb(W, K, 23*265, 50, 3).to(device)\n",
    "        elif model_name == 'Poly':\n",
    "            model = DGCN_Poly(W, K, 23*265, 50, 3).to(device)\n",
    "        elif model_name == 'ChebyshevNormMod':\n",
    "            model = DGCN_ChebNormMod(W, K, 23*265, 50, 3).to(device)\n",
    "        elif model_name == 'ChebyshevGCN':\n",
    "            model = GCNCheb(W, K, 23*265, 50, 3).to(device)\n",
    "        else :\n",
    "            raise ValueError('Please select Cheby, Poly, ChebyshevNormMod, PolyNormMod')\n",
    "            \n",
    "    if (feature == 'asm'):\n",
    "        if model_name == 'Chebyshev':\n",
    "            model = DGCN_Cheb(W, K, 54*265, 50, 3).to(device)\n",
    "        elif model_name == 'Poly':\n",
    "            model = DGCN_Poly(W, K, 54*265, 50, 3).to(device)\n",
    "        elif model_name == 'ChebyshevNormMod':\n",
    "            model = DGCN_ChebNormMod(W, K, 54*265, 50, 3).to(device)\n",
    "        elif model_name == 'ChebyshevGCN':\n",
    "            model = GCNCheb(W, K, 54*265, 50, 3).to(device)\n",
    "        else :\n",
    "            raise ValueError('Please select Cheby, Poly, ChebyshevNormMod, PolyNormMod')\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92bb7a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(W_de), type(W_dasm), type(W_dcau), type(W_asm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769fc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Model: Chebyshev--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_names = ['Chebyshev', 'Poly', 'ChebyshevNormMod', 'ChebyshevGCN']\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    print('-----------Model: {}--------------'.format(model_name))\n",
    "    print('')\n",
    "    \n",
    "    #ASM\n",
    "    t1 = time.time()\n",
    "    asm_glob_test_acc = get_accuracy_results('asm', W_asm, model_name)\n",
    "    print('-------Temps d\\'exécution -----------')\n",
    "    print(t2 - t1)\n",
    "    print('')\n",
    "    res_asm = eh_merce_function('asm', asm_glob_test_acc)\n",
    "    \n",
    "    #DCAU\n",
    "    t1 = time.time()\n",
    "    dcau_glob_test_acc = get_accuracy_results('dcau', W_dcau, model_name)\n",
    "    print('-------Temps d\\'exécution -----------')\n",
    "    print(t2 - t1)\n",
    "    print('')\n",
    "    res_dcau = eh_merce_function('dcau', dcau_glob_test_acc)\n",
    "    print('')\n",
    "    \n",
    "    #DE\n",
    "    t1 = time.time()\n",
    "    de_glob_test_acc = get_accuracy_results('de', W_de, model_name)\n",
    "    t2 = time.time()\n",
    "    print('-------Temps d\\'exécution -----------')\n",
    "    print(t2 - t1)\n",
    "    print('')\n",
    "    res_de = eh_merce_function('de', de_glob_test_acc)\n",
    "    print('')\n",
    "\n",
    "    #PSD\n",
    "    t1 = time.time()\n",
    "    psd_glob_test_acc = get_accuracy_results('psd', W_de, model_name)\n",
    "    print('-------Temps d\\'exécution -----------')\n",
    "    print(t2 - t1)\n",
    "    print('')\n",
    "    res_psd = eh_merce_function('psd', psd_glob_test_acc)\n",
    "    print('')\n",
    "\n",
    "    #DASM\n",
    "    t1 = time.time()\n",
    "    dasm_glob_test_acc = get_accuracy_results('dasm', W_dasm, model_name)\n",
    "    print('-------Temps d\\'exécution -----------')\n",
    "    print(t2 - t1)\n",
    "    print('')\n",
    "    res_dasm = eh_merce_function('dasm', dasm_glob_test_acc)\n",
    "    print('')\n",
    "\n",
    "    #RASM\n",
    "    t1 = time.time()\n",
    "    rasm_glob_test_acc = get_accuracy_results('rasm', W_dasm, model_name)\n",
    "    print('-------Temps d\\'exécution -----------')\n",
    "    print(t2 - t1)\n",
    "    print('')\n",
    "    res_rasm = eh_merce_function('rasm', rasm_glob_test_acc)\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeba1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
